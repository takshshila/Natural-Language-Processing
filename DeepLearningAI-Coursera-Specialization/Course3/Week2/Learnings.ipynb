{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden State Activation\n",
    "\n",
    "![vanilla rnn](images/vanilla_rnn.PNG)\n",
    "\n",
    "\n",
    "This is the hidden state activation function for a vanilla RNN.\n",
    "\n",
    "$h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)$                                                    \n",
    "\n",
    "Which is another way of writing this:         \n",
    "\n",
    "$h^{<t>}=g(W_{hh}h^{<t-1>} \\oplus W_{hx}x^{<t>} + b_h)$                                        \n",
    "\n",
    "Where \n",
    "\n",
    "- $W_{h}$ in the first formula is denotes the *horizontal* concatenation of $W_{hh}$ and $W_{hx}$ from the second formula.\n",
    "\n",
    "- $W_{h}$ in the first formula is then multiplied by $[h^{<t-1>},x^{<t>}]$, another concatenation of parameters from the second formula but this time in a different direction, i.e *vertical*!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining (Concatenation)\n",
    "\n",
    "### Weights\n",
    "\n",
    "A join along the vertical boundary is called a *horizontal concatenation* or *horizontal stack*. \n",
    "\n",
    "Visually, it looks like this:- $W_h = \\left [ W_{hh} \\ | \\ W_{hx} \\right ]$\n",
    "\n",
    "I'll show you two different ways to achieve this using numpy.\n",
    "\n",
    "__Note: The values used to populate the arrays, below, have been chosen to aid in visual illustration only. They are NOT what you'd expect to use building a model, which would typically be random variables instead.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Data --\n",
      "\n",
      "w_hh :\n",
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "w_hh shape : (3, 2) \n",
      "\n",
      "w_hx :\n",
      "[[9 9 9]\n",
      " [9 9 9]\n",
      " [9 9 9]]\n",
      "w_hx shape : (3, 3) \n",
      "\n",
      "-- Joining --\n",
      "\n",
      "option 1 : concatenate\n",
      "\n",
      "w_h :\n",
      "[[1 1 9 9 9]\n",
      " [1 1 9 9 9]\n",
      " [1 1 9 9 9]]\n",
      "w_h shape : (3, 5) \n",
      "\n",
      "option 2 : hstack\n",
      "\n",
      "w_h :\n",
      "[[1 1 9 9 9]\n",
      " [1 1 9 9 9]\n",
      " [1 1 9 9 9]]\n",
      "w_h shape : (3, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create some dummy data\n",
    "\n",
    "w_hh = np.full((3, 2), 1)  # illustration purposes only, returns an array of size 3x2 filled with all 1s\n",
    "w_hx = np.full((3, 3), 9)  # illustration purposes only, returns an array of size 3x3 filled with all 9s\n",
    "\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n",
    "# w_hh = np.random.standard_normal((3,2))\n",
    "# w_hx = np.random.standard_normal((3,3))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"-- Data --\\n\")\n",
    "print(\"w_hh :\")\n",
    "print(w_hh)\n",
    "print(\"w_hh shape :\", w_hh.shape, \"\\n\")\n",
    "print(\"w_hx :\")\n",
    "print(w_hx)\n",
    "print(\"w_hx shape :\", w_hx.shape, \"\\n\")\n",
    "\n",
    "# Joining the arrays\n",
    "print(\"-- Joining --\\n\")\n",
    "# Option 1: concatenate - horizontal\n",
    "w_h1 = np.concatenate((w_hh, w_hx), axis=1)\n",
    "print(\"option 1 : concatenate\\n\")\n",
    "print(\"w_h :\")\n",
    "print(w_h1)\n",
    "print(\"w_h shape :\", w_h1.shape, \"\\n\")\n",
    "\n",
    "# Option 2: hstack\n",
    "w_h2 = np.hstack((w_hh, w_hx))\n",
    "print(\"option 2 : hstack\\n\")\n",
    "print(\"w_h :\")\n",
    "print(w_h2)\n",
    "print(\"w_h shape :\", w_h2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden State & Inputs\n",
    "Joining along a horizontal boundary is called a vertical concatenation or vertical stack. Visually it looks like this:\n",
    "\n",
    "$[h^{<t-1>},x^{<t>}] = \\left[ \\frac{h^{<t-1>}}{x^{<t>}} \\right]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Data --\n",
      "\n",
      "h_t_prev :\n",
      "[[1]\n",
      " [1]]\n",
      "h_t_prev shape : (2, 1) \n",
      "\n",
      "x_t :\n",
      "[[9]\n",
      " [9]\n",
      " [9]]\n",
      "x_t shape : (3, 1) \n",
      "\n",
      "-- Joining --\n",
      "\n",
      "option 1 : concatenate\n",
      "\n",
      "ax_1 :\n",
      "[[1]\n",
      " [1]\n",
      " [9]\n",
      " [9]\n",
      " [9]]\n",
      "ax_1 shape : (5, 1) \n",
      "\n",
      "option 2 : vstack\n",
      "\n",
      "ax_2 :\n",
      "[[1]\n",
      " [1]\n",
      " [9]\n",
      " [9]\n",
      " [9]]\n",
      "ax_2 shape : (5, 1)\n"
     ]
    }
   ],
   "source": [
    "# Create some more dummy data\n",
    "h_t_prev = np.full((2, 1), 1)  # illustration purposes only, returns an array of size 2x1 filled with all 1s\n",
    "x_t = np.full((3, 1), 9)       # illustration purposes only, returns an array of size 3x1 filled with all 9s\n",
    "\n",
    "# Try using some random initializations, though it will obfuscate the join. eg: uncomment these lines\n",
    "\n",
    "### START CODE HERE ###\n",
    "# h_t_prev = np.random.standard_normal((2,1))\n",
    "# x_t = np.random.standard_normal((3,1))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"-- Data --\\n\")\n",
    "print(\"h_t_prev :\")\n",
    "print(h_t_prev)\n",
    "print(\"h_t_prev shape :\", h_t_prev.shape, \"\\n\")\n",
    "print(\"x_t :\")\n",
    "print(x_t)\n",
    "print(\"x_t shape :\", x_t.shape, \"\\n\")\n",
    "\n",
    "# Joining the arrays\n",
    "print(\"-- Joining --\\n\")\n",
    "\n",
    "# Option 1: concatenate - vertical\n",
    "ax_1 = np.concatenate(\n",
    "    (h_t_prev, x_t), axis=0\n",
    ")  # note the difference in axis parameter vs earlier\n",
    "print(\"option 1 : concatenate\\n\")\n",
    "print(\"ax_1 :\")\n",
    "print(ax_1)\n",
    "print(\"ax_1 shape :\", ax_1.shape, \"\\n\")\n",
    "\n",
    "# Option 2: vstack\n",
    "ax_2 = np.vstack((h_t_prev, x_t))\n",
    "print(\"option 2 : vstack\\n\")\n",
    "print(\"ax_2 :\")\n",
    "print(ax_2)\n",
    "print(\"ax_2 shape :\", ax_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Formulas\n",
    "Now you know how to do the concatenations, horizontal and vertical, lets verify if the two formulas produce the same result.\n",
    "\n",
    "__Formula 1:__ $h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)$ \n",
    "\n",
    "__Formula 2:__ $h^{<t>}=g(W_{hh}h^{<t-1>} \\oplus W_{hx}x^{<t>} + b_h)$\n",
    "\n",
    "\n",
    "To prove:- __Formula 1__ $\\Leftrightarrow$ __Formula 2__\n",
    "\n",
    "We will ignore the bias term $b_h$ and the activation function $g(\\ )$ because the transformation will be identical for each formula. So what we really want to compare is the result of the following parameters inside each formula:\n",
    "\n",
    "$W_{h}[h^{<t-1>},x^{<t>}] \\quad \\Leftrightarrow \\quad W_{hh}h^{<t-1>} \\oplus W_{hx}x^{<t>} $\n",
    "\n",
    "We'll see how to do this using matrix multiplication combined with the data and techniques (stacking/concatenating) from above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Results --\n",
      "\n",
      "Formula 1\n",
      "Term1:\n",
      " [[1 1 9 9 9]\n",
      " [1 1 9 9 9]\n",
      " [1 1 9 9 9]]\n",
      "Term2:\n",
      " [[1]\n",
      " [1]\n",
      " [9]\n",
      " [9]\n",
      " [9]]\n",
      "Output:\n",
      "[[245]\n",
      " [245]\n",
      " [245]]\n",
      "\n",
      "Formula 2\n",
      "Term1:\n",
      " [[2]\n",
      " [2]\n",
      " [2]]\n",
      "Term2:\n",
      " [[243]\n",
      " [243]\n",
      " [243]]\n",
      "\n",
      "Output:\n",
      "[[245]\n",
      " [245]\n",
      " [245]] \n",
      "\n",
      "-- Verify --\n",
      "Results are the same : True\n",
      "Formula 1 Output:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Formula 2 Output:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "Results after activation are the same : True\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "\n",
    "w_hh = np.full((3, 2), 1)  # returns an array of size 3x2 filled with all 1s\n",
    "w_hx = np.full((3, 3), 9)  # returns an array of size 3x3 filled with all 9s\n",
    "h_t_prev = np.full((2, 1), 1)  # returns an array of size 2x1 filled with all 1s\n",
    "x_t = np.full((3, 1), 9)       # returns an array of size 3x1 filled with all 9s\n",
    "\n",
    "\n",
    "# If you want to randomize the values, uncomment the next 4 lines\n",
    "\n",
    "# w_hh = np.random.standard_normal((3,2))\n",
    "# w_hx = np.random.standard_normal((3,3))\n",
    "# h_t_prev = np.random.standard_normal((2,1))\n",
    "# x_t = np.random.standard_normal((3,1))\n",
    "\n",
    "# Results\n",
    "print(\"-- Results --\")\n",
    "# Formula 1\n",
    "stack_1 = np.hstack((w_hh, w_hx))\n",
    "stack_2 = np.vstack((h_t_prev, x_t))\n",
    "\n",
    "print(\"\\nFormula 1\")\n",
    "print(\"Term1:\\n\",stack_1)\n",
    "print(\"Term2:\\n\",stack_2)\n",
    "formula_1 = np.matmul(np.hstack((w_hh, w_hx)), np.vstack((h_t_prev, x_t)))\n",
    "print(\"Output:\")\n",
    "print(formula_1)\n",
    "\n",
    "# Formula 2\n",
    "mul_1 = np.matmul(w_hh, h_t_prev)\n",
    "mul_2 = np.matmul(w_hx, x_t)\n",
    "print(\"\\nFormula 2\")\n",
    "print(\"Term1:\\n\",mul_1)\n",
    "print(\"Term2:\\n\",mul_2)\n",
    "\n",
    "formula_2 = np.matmul(w_hh, h_t_prev) + np.matmul(w_hx, x_t)\n",
    "print(\"\\nOutput:\")\n",
    "print(formula_2, \"\\n\")\n",
    "\n",
    "# Verification \n",
    "# np.allclose - to check if two arrays are elementwise equal upto certain tolerance, here  \n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n",
    "\n",
    "print(\"-- Verify --\")\n",
    "print(\"Results are the same :\", np.allclose(formula_1, formula_2))\n",
    "\n",
    "### START CODE HERE ###\n",
    "# # Try adding a sigmoid activation function and bias term as a final check\n",
    "# # Activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# # Bias and check\n",
    "b = np.random.standard_normal((formula_1.shape[0],1))\n",
    "print(\"Formula 1 Output:\\n\",sigmoid(formula_1+b))\n",
    "print(\"Formula 2 Output:\\n\",sigmoid(formula_2+b))\n",
    "\n",
    "all_close = np.allclose(sigmoid(formula_1+b), sigmoid(formula_2+b))\n",
    "print(\"Results after activation are the same :\",all_close)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with JAX numpy and calculating perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takshshilarawat/opt/anaconda3/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "\n",
    "# Setting random seeds\n",
    "trax.supervised.trainer_lib.init_random_number_generators(32)\n",
    "numpy.random.seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regular numpy array looks like this:\n",
      "\n",
      " [[0.85888927 0.37271115 0.55512878 0.95565655 0.7366696  0.81620514\n",
      "  0.10108656 0.92848807 0.60910917 0.59655344]\n",
      " [0.09178413 0.34518624 0.66275252 0.44171349 0.55148779 0.70371249\n",
      "  0.58940123 0.04993276 0.56179184 0.76635847]\n",
      " [0.91090833 0.09290995 0.90252139 0.46096041 0.45201847 0.99942549\n",
      "  0.16242374 0.70937058 0.16062408 0.81077677]\n",
      " [0.03514717 0.53488673 0.16650012 0.30841038 0.04506241 0.23857613\n",
      "  0.67483453 0.78238275 0.69520163 0.32895445]\n",
      " [0.49403187 0.52412136 0.29854125 0.46310814 0.98478429 0.50113492\n",
      "  0.39807245 0.72790532 0.86333097 0.02616954]]\n",
      "\n",
      "It is of type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "numpy_array = numpy.random.random((5,10))\n",
    "print(f\"The regular numpy array looks like this:\\n\\n {numpy_array}\\n\")\n",
    "print(f\"It is of type: {type(numpy_array)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trax numpy array looks like this:\n",
      "\n",
      " [[0.8588893  0.37271115 0.55512875 0.9556565  0.7366696  0.81620514\n",
      "  0.10108656 0.9284881  0.60910916 0.59655344]\n",
      " [0.09178413 0.34518623 0.6627525  0.44171348 0.5514878  0.70371246\n",
      "  0.58940125 0.04993276 0.56179184 0.7663585 ]\n",
      " [0.91090834 0.09290995 0.9025214  0.46096042 0.45201847 0.9994255\n",
      "  0.16242374 0.7093706  0.16062407 0.81077677]\n",
      " [0.03514718 0.5348867  0.16650012 0.30841038 0.04506241 0.23857613\n",
      "  0.67483455 0.7823827  0.69520164 0.32895446]\n",
      " [0.49403188 0.52412134 0.29854125 0.46310815 0.9847843  0.50113493\n",
      "  0.39807245 0.72790533 0.86333096 0.02616954]]\n",
      "\n",
      "It is of type: <class 'jax.interpreters.xla.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "trax_numpy_array = np.array(numpy_array)\n",
    "print(f\"The trax numpy array looks like this:\\n\\n {trax_numpy_array}\\n\")\n",
    "print(f\"It is of type: {type(trax_numpy_array)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Perplexity\n",
    "\n",
    "\n",
    "The perplexity is a metric that measures how well a probability model predicts a sample and it is commonly used to evaluate language models. It is defined as: \n",
    "\n",
    "$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\n",
    "\n",
    "As an implementation hack, you would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). You should also take care of the padding, since you do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good). The algebra behind this process is explained next:\n",
    "\n",
    "\n",
    "$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$ \n",
    "\n",
    "$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\n",
    "$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\n",
    "$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions has shape: (32, 64, 256)\n",
      "targets has shape: (32, 64)\n"
     ]
    }
   ],
   "source": [
    "from trax import layers as tl\n",
    "\n",
    "# Load from .npy files\n",
    "predictions = numpy.load('predictions.npy')\n",
    "targets = numpy.load('targets.npy')\n",
    "\n",
    "# Cast to jax.interpreters.xla.DeviceArray\n",
    "predictions = np.array(predictions)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# Print shapes\n",
    "print(f'predictions has shape: {predictions.shape}')\n",
    "print(f'targets has shape: {targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped_targets has shape: (32, 64, 256)\n"
     ]
    }
   ],
   "source": [
    "reshaped_targets = tl.one_hot(targets, predictions.shape[-1]) #trax's one_hot function takes the input as one_hot(x, n_categories, dtype=optional)\n",
    "print(f'reshaped_targets has shape: {reshaped_targets.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_log_ppx = np.sum(predictions * reshaped_targets, axis= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non_pad has shape: (32, 64)\n",
      "\n",
      "non_pad looks like this: \n",
      "\n",
      " [[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "non_pad = 1.0 - np.equal(targets, 0)\n",
    "print(f'non_pad has shape: {non_pad.shape}\\n')\n",
    "print(f'non_pad looks like this: \\n\\n {non_pad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real perplexity still has shape: (32, 64)\n"
     ]
    }
   ],
   "source": [
    "real_log_ppx = total_log_ppx * non_pad\n",
    "print(f'real perplexity still has shape: {real_log_ppx.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log perplexity tensor before filtering padding: \n",
      "\n",
      " [[ -5.396545    -1.0311184   -0.66916656 ... -22.37673    -23.18771\n",
      "  -21.843483  ]\n",
      " [ -4.5857706   -1.1341286   -8.538033   ... -20.15686    -26.837097\n",
      "  -23.57502   ]\n",
      " [ -5.2223887   -1.2824144   -0.17312431 ... -21.328228   -19.854412\n",
      "  -33.88444   ]\n",
      " ...\n",
      " [ -5.396545   -17.291681    -4.360766   ... -20.825802   -21.065838\n",
      "  -22.443115  ]\n",
      " [ -5.9313164  -14.247417    -0.2637329  ... -26.743248   -18.38433\n",
      "  -22.355278  ]\n",
      " [ -5.670536    -0.10595131   0.         ... -23.332523   -28.087376\n",
      "  -23.878807  ]]\n",
      "\n",
      "log perplexity tensor after filtering padding: \n",
      "\n",
      " [[ -5.396545    -1.0311184   -0.66916656 ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -4.5857706   -1.1341286   -8.538033   ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -5.2223887   -1.2824144   -0.17312431 ...  -0.          -0.\n",
      "   -0.        ]\n",
      " ...\n",
      " [ -5.396545   -17.291681    -4.360766   ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -5.9313164  -14.247417    -0.2637329  ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -5.670536    -0.10595131   0.         ...  -0.          -0.\n",
      "   -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'log perplexity tensor before filtering padding: \\n\\n {total_log_ppx}\\n')\n",
    "print(f'log perplexity tensor after filtering padding: \\n\\n {real_log_ppx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of the model are respectively: 2.3281209468841553 and 10.258646965026855\n"
     ]
    }
   ],
   "source": [
    "log_ppx = np.sum(real_log_ppx) / np.sum(non_pad)\n",
    "log_ppx = -log_ppx\n",
    "print(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log perplexity tensor before filtering padding: \n",
      "\n",
      " [[ -5.396545    -1.0311184   -0.66916656 ... -22.37673    -23.18771\n",
      "  -21.843483  ]\n",
      " [ -4.5857706   -1.1341286   -8.538033   ... -20.15686    -26.837097\n",
      "  -23.57502   ]\n",
      " [ -5.2223887   -1.2824144   -0.17312431 ... -21.328228   -19.854412\n",
      "  -33.88444   ]\n",
      " ...\n",
      " [ -5.396545   -17.291681    -4.360766   ... -20.825802   -21.065838\n",
      "  -22.443115  ]\n",
      " [ -5.9313164  -14.247417    -0.2637329  ... -26.743248   -18.38433\n",
      "  -22.355278  ]\n",
      " [ -5.670536    -0.10595131   0.         ... -23.332523   -28.087376\n",
      "  -23.878807  ]]\n",
      "\n",
      "log perplexity tensor after filtering padding: \n",
      "\n",
      " [[ -5.396545    -1.0311184   -0.66916656 ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -4.5857706   -1.1341286   -8.538033   ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -5.2223887   -1.2824144   -0.17312431 ...  -0.          -0.\n",
      "   -0.        ]\n",
      " ...\n",
      " [ -5.396545   -17.291681    -4.360766   ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -5.9313164  -14.247417    -0.2637329  ...  -0.          -0.\n",
      "   -0.        ]\n",
      " [ -5.670536    -0.10595131   0.         ...  -0.          -0.\n",
      "   -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(f'log perplexity tensor before filtering padding: \\n\\n {total_log_ppx}\\n')\n",
    "print(f'log perplexity tensor after filtering padding: \\n\\n {real_log_ppx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log perplexity and perplexity of the model are respectively: 2.3281209468841553 and 10.258646965026855\n"
     ]
    }
   ],
   "source": [
    "log_ppx = np.sum(real_log_ppx) / np.sum(non_pad)\n",
    "log_ppx = -log_ppx\n",
    "print(f'The log perplexity and perplexity of the model are respectively: {log_ppx} and {np.exp(log_ppx)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNNs, GRUs and the `scan` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): # Sigmoid function\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward method for vanilla RNNs and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)                 # Random seed, so your results match ours\n",
    "emb = 128                       # Embedding size\n",
    "T = 256                         # Number of variables in the sequences\n",
    "h_dim = 16                      # Hidden state dimension\n",
    "h_0 = np.zeros((h_dim, 1))      # Initial hidden state\n",
    "# Random initialization of weights and biases\n",
    "w1 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w2 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w3 = random.standard_normal((h_dim, emb+h_dim))\n",
    "b1 = random.standard_normal((h_dim, 1))\n",
    "b2 = random.standard_normal((h_dim, 1))\n",
    "b3 = random.standard_normal((h_dim, 1))\n",
    "X = random.standard_normal((T, emb, 1))\n",
    "weights = [w1, w2, w3, b1, b2, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward method for vanilla RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.PNG\" width=\"400\"/>\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "h^{<t>}=g(W_{h}[h^{<t-1>},x^{<t>}] + b_h)\n",
    "\\label{eq: htRNN}\n",
    "\\end{equation}\n",
    "    \n",
    "\\begin{equation}\n",
    "\\hat{y}^{<t>}=g(W_{yh}h^{<t>} + b_y)\n",
    "\\label{eq: ytRNN}\n",
    "\\end{equation}\n",
    "\n",
    "where $[h^{<t-1>},x^{<t>}]$ means that $h^{<t-1>}$ and $x^{<t>}$ are concatenated together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_V_RNN(inputs, weights): # Forward propagation for a a single vanilla RNN cell\n",
    "    x, h_t = inputs\n",
    "\n",
    "    # weights.\n",
    "    wh, _, _, bh, _, _ = weights\n",
    "\n",
    "    # new hidden state\n",
    "    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n",
    "    h_t = sigmoid(h_t)\n",
    "\n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward method for GRUs\n",
    "\n",
    "\n",
    "<img src=\"images/GRU.PNG\" width=\"400\"/>\n",
    "\n",
    "GRUs have relevance $\\Gamma_r$ and update $\\Gamma_u$ gates that control how the hidden state $h^{<t>}$ is updated on every time step. With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences. The equations needed for the forward method in GRUs are provided below: \n",
    "\n",
    "\\begin{equation}\n",
    "\\Gamma_r=\\sigma{(W_r[h^{<t-1>}, x^{<t>}]+b_r)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Gamma_u=\\sigma{(W_u[h^{<t-1>}, x^{<t>}]+b_u)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "c^{<t>}=\\tanh{(W_h[\\Gamma_r*h^{<t-1>},x^{<t>}]+b_h)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h^{<t>}=\\Gamma_u*c^{<t>}+(1-\\Gamma_u)*h^{<t-1>}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_GRU(inputs, weights): # Forward propagation for a single GRU cell\n",
    "    x, h_t = inputs\n",
    "\n",
    "    # weights.\n",
    "    wu, wr, wc, bu, br, bc = weights\n",
    "\n",
    "    # Update gate\n",
    "    ### START CODE HERE (1-2 lINES) ###\n",
    "    u = np.dot(wu, np.concatenate([h_t, x])) + bu\n",
    "    u = sigmoid(u)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Relevance gate\n",
    "    ### START CODE HERE (1-2 lINES) ###\n",
    "    r = np.dot(wr, np.concatenate([h_t, x])) + br\n",
    "    r = sigmoid(u)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Candidate hidden state \n",
    "    ### START CODE HERE (1-2 lINES) ###\n",
    "    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n",
    "    c = np.tanh(c)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # New Hidden state h_t\n",
    "    h_t = u* c + (1 - u)* h_t\n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.77779014e-01],\n",
       "       [-9.97986240e-01],\n",
       "       [-5.19958083e-01],\n",
       "       [-9.99999886e-01],\n",
       "       [-9.99707004e-01],\n",
       "       [-3.02197037e-04],\n",
       "       [-9.58733503e-01],\n",
       "       [ 2.10804828e-02],\n",
       "       [ 9.77365398e-05],\n",
       "       [ 9.99833090e-01],\n",
       "       [ 1.63200940e-08],\n",
       "       [ 8.51874303e-01],\n",
       "       [ 5.21399924e-02],\n",
       "       [ 2.15495959e-02],\n",
       "       [ 9.99878828e-01],\n",
       "       [ 9.77165472e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_GRU([X[1],h_0], weights)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the `scan` function\n",
    "\n",
    "- `fn` : the function to be called recurrently (i.e. `forward_GRU`)\n",
    "- `elems` : the list of inputs for each time step (`X`)\n",
    "- `weights` : the parameters needed to compute `fn`\n",
    "- `h_0` : the initial hidden state\n",
    "â€‹\n",
    "`scan` goes through all the elements `x` in `elems`, calls the function `fn` with arguments ([`x`, `h_t`],`weights`), stores the computed hidden state `h_t` and appends the result to a list `ys`. Complete the following cell by calling `fn` with arguments ([`x`, `h_t`],`weights`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(fn, elems, weights, h_0=None): # Forward propagation for RNNs\n",
    "    h_t = h_0\n",
    "    ys = []\n",
    "    for x in elems:\n",
    "        ### START CODE HERE (1 lINE) ###\n",
    "        y, h_t = fn([x, h_t], weights)\n",
    "        ### END CODE HERE ###\n",
    "        ys.append(y)\n",
    "    return ys, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison between vanilla RNNs and GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 5.13ms to run the forward method for the vanilla RNN.\n"
     ]
    }
   ],
   "source": [
    "# vanilla RNNs\n",
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_V_RNN, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "RNN_time=(toc-tic)*1000\n",
    "print (f\"It took {RNN_time:.2f}ms to run the forward method for the vanilla RNN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 10.59ms to run the forward method for the GRU.\n"
     ]
    }
   ],
   "source": [
    "# GRUs\n",
    "tic = perf_counter()\n",
    "ys, h_T = scan(forward_GRU, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "GRU_time=(toc-tic)*1000\n",
    "print (f\"It took {GRU_time:.2f}ms to run the forward method for the GRU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
