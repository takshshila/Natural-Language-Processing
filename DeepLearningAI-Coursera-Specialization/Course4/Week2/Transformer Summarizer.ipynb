{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset cnn_dailymail/plain_text/3.0.0 (download: 558.32 MiB, generated: 1.27 GiB, total: 1.82 GiB) to /Users/takshshilarawat/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bada30f7171470f85bdadafd39296ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5889ad8d0aac42d6907a0dd1c684a83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef94f5dfe3d44d98d51abfbbc1cf3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /Users/takshshilarawat/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0.incompleteNE8IVC/cnn_dailymail-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a6a86047974c81a2b6f41b8756c3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=287113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/takshshilarawat/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0.incompleteNE8IVC/cnn_dailymail-validation.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a526de0bfbd4e4d9530f5ee690e0679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13368.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Shuffling and writing examples to /Users/takshshilarawat/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0.incompleteNE8IVC/cnn_dailymail-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0035e8b55d44f7b38463403b25f388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cnn_dailymail downloaded and prepared to /Users/takshshilarawat/tensorflow_datasets/cnn_dailymail/plain_text/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
      "\r"
     ]
    }
   ],
   "source": [
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str, EOS=1):\n",
    " \n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "\n",
    "input_pipeline = trax.data.Serial(\n",
    "    \n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='summarize32k.subword.subwords'),\n",
    "    preprocess,\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " By . Daily Mail Reporter . UPDATED: . 09:53 EST, 12 January 2012 . A\n",
      "motorist was stunned when he discovered double yellow lines had been\n",
      "painted underneath his car while it was parked - and then given a\n",
      "ticket. Flecks of yellow paint were even sprayed on the bumper of\n",
      "Patrick McCrystal's car as the lines were painted under the front of\n",
      "it in Kedleston Street, Derby. The 49-year-old had parked his Ford\n",
      "Fiesta in the street near to a Co-operative store and a petrol\n",
      "station, where he works, for three years. Stunned: Patrick McCrystal\n",
      "with his Ford Fiesta, which was given a ticket after council workmen\n",
      "had sprayed yellow lines under the car while it was parked legally .\n",
      "When he parked for his 2pm shift, he noticed new yellow lines had been\n",
      "painted across a housing block entrance. But there was a gap between\n",
      "those lines and existing ones in the street, so Mr McCrystal parked\n",
      "there, in his usual spot. Hours later, a colleague on his dinner break\n",
      "saw that extra lines had been painted below the front of Mr\n",
      "McCrystal's car. And, when he went out, Mr McCrystal discovered he had\n",
      "been given a parking ticket. Tell-tale sign: The workmen left flecks\n",
      "of paint on the bumper of the car as they sprayed underneath . He\n",
      "said: 'There is even paint on my front bumper where they've caught it\n",
      "reaching under to paint the lines. 'There were no lines there when I\n",
      "parked. I can't believe they've done it.' The new lines extended from\n",
      "existing double yellow lines which had previously ended about two feet\n",
      "from Mr McCrystal's front bumper. Apology: The ticket left on Mr\n",
      "McCrystal's window. Derby City Council blamed a breakdown in\n",
      "communication and has now revoked the £70 fine . Now a Derby City\n",
      "Council official has said the £70 fine had been issued in error and\n",
      "would be revoked. Mr McCrystal, from Derby, said notices had been put\n",
      "up several weeks ago saying parking restrictions would be introduced\n",
      "but they did not say when. He said: 'If they were going to paint\n",
      "yellow lines then they should have put up cones to stop people\n",
      "parking. 'When I pulled up, I saw the gap between the old lines and\n",
      "the new ones and thought it was okay to park there. Nothing gave me\n",
      "the impression I couldn't.' A passer-by said council staff had been\n",
      "angered they could not paint the lines because of the parked car.\n",
      "David Gartside, head of traffic and transport at the authority, said:\n",
      "'We can confirm that lining was painted on Kedleston Street this\n",
      "afternoon after the vehicle had parked and that a parking ticket was\n",
      "issued to the vehicle in error. 'It appears that there was a\n",
      "communication breakdown between our contractors undertaking the lining\n",
      "work and our enforcement officers. 'The ticket will be revoked and we\n",
      "apologise for any inconvenience caused.'<EOS><pad>Theyeven sprayed\n",
      "paint on his Ford Fiesta in calamitous giveaway .<EOS>\n"
     ]
    }
   ],
   "source": [
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,    8,    4,    2, 1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1727)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch, _, mask_batch = next(train_batch_stream)\n",
    "\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  380   527   213   296 29725     4     5  2448  3620 15124   902    39\n",
      "  1151   669 27439  6050 13459  1628  3528  1879 29725   391   592   166\n",
      "   527    95     6  1260  2942   578     3   198    39  1151    92  6164\n",
      "   132   181    64   527   805 29725     4     5  2840   132   401   509\n",
      "   320 20669   320  3636 15052   578   770   527   213   947     3     9\n",
      " 24046  1041    78    36   527   213 13965  1229  1536   390   527   213\n",
      "   104     2   412  2659   527   101  1124   320   955   278   102  4956\n",
      "   228  1019  4078     3  9175  6051     4   246  1019   846   379  2165\n",
      "   132   186    64   527   401  9133  2840   947    18    46 19715   592\n",
      "     2   103    23    46  1595   379 11423 17805   232    11 13049  7844\n",
      "   809   213   401   947     2    36   527   213 13965  1229   132   213\n",
      "   296     2  4872   447  1435 19715   379     9 26588   400   809   213\n",
      "   947     2  1480   229  2232   691  3636 15052     2    39  2897   161\n",
      "  1427   320  1536    78   644  2836     2   758 17062 21675     5     2\n",
      "  1711  1187   186  1109  1522   447     3   644  2836 21675     5   133\n",
      "   213  9314    78    50  1438  7356  2801     2  4872   103  9961  6672\n",
      "   320  8977    31  1536   122   498     3    52    43   127   285    28\n",
      "  8010  4944  1253   229   950   144   759   186    39  1151   133   336\n",
      "   412  1132   412   498     3 11124   185    18    46  9961    41   317\n",
      "   320   787   181  4185    31  3007   809  5527 22291   713   132   770\n",
      "   401     2  1248   809   518    36   425   527  1973   166   527   213\n",
      " 11833  1985   578   132   213  8584 15270    20   201     3    27  2012\n",
      "   332   320   186  1838   213   947    39   273  2504   412  2935    78\n",
      "  1681     2  1248  6164   669   391  2252    61   320   247  1170  1121\n",
      "    74  1923  1838   805     7     5  2840     3  2764   736 26956  1635\n",
      " 23280     2   213  1314  8417  3250  3957     2   897   213 20669   412\n",
      "   669 27439  6050 13459  1628 21847 12498   113 29725     3   644  2836\n",
      "   133   213  9314    78    31  1438   186 14422  1407  1019 24046    78\n",
      "  2754   229   117   163   674   139  5955  1536   194  1820   355   213\n",
      "  4078  2033    80   207  1355    91   527  9753  1838  6672   285  1651\n",
      " 29725     4     5  8102   720  3528   669 27439  6050 13459  1628 10409\n",
      "   246 29725   391    95   213  4078   186   119  2006  5069     3     9\n",
      " 16130   229 26417  5344   691   213   503   285   213   515  2836  4209\n",
      "  2939   229   674  1879  1019  2942   578    95   213  4078   422     3\n",
      "     9  1626   229 10409   111   401  5331  5847   186 11213   686 11213\n",
      " 23597  8390   132  4077 10716  8497  4885   353  2613     2   186    43\n",
      "  1879   111 20847  4515   186 22165    14   353  1681     3  2764   736\n",
      " 20586     5 12553    17    31 21327     4  1248   213   805 29725     4\n",
      "     5  2840 11298    78   299   564     2  1821  3636 15052    40   669\n",
      " 27439  6050 13459  1628 11333   320   124 29725     3     9   947   229\n",
      "   213  1698 23589   527   213   644  2836  4209  2939     2  1480   858\n",
      "   447   320   407  1397   176 11469     2 10665   186  7568     3   644\n",
      "  2836 14380     2   915 13493 24923    20     2   127  3611  3636 15052\n",
      "    23 14422  1407   320  6672  1019   213 13765 20669   320    31  1536\n",
      "  1420    78  1895   412    28   581   527   213 11833 19962    16  2942\n",
      "   578  1782   644  2836   229   881 12844   403  1019   213 13252  8129\n",
      "  2687   320    50  2982   412    28   581     2    78  2754    51   288\n",
      "   229   163   674   139  5955  1536   194  1820   355   213  4078  2033\n",
      "     3 17977   127   285   213   947    62  3528  1151  1879   669   391\n",
      "   509   320 20669   320   578   770   527   213   947 11969     7   174\n",
      "  2982 13951    16   320   787   181  4185    31  3007   809   805     7\n",
      "     5  2840     2  4245   170  1151   411   320 14958   893  1536  1420\n",
      "   320   673  1681   181  2613  1782   129     7   165   477   800    95\n",
      "   213  5069   422   320   191   213  1015 25179   320    89  4944  1253\n",
      "   412    28  7807     2   186   320   430   412   196   215   412    51\n",
      "    49  2002   276 15052   127    78    50  1438   285   447   320   186\n",
      "  1838   805     7     5  2840    62  1151   117  2111 17201    17    80\n",
      " 11822     3    52   127    77    39  1151    92  6164   171  6774   410\n",
      " 11822   186    95   213  3853     2   125  6164    39   787   181 11791\n",
      "     4   809  7547 14716    58     2 14248  5950     2 16371  4628   657\n",
      "   181  5527 22291   713     3  5674     2   758 17062 21675     5    39\n",
      "   655    72  6164   132   193  1757   193   194   320   186  1838   401\n",
      "   383 25152 16383   597  1382   527   401  9133  2840     3  1109  1522\n",
      "  1435 20246    16    87  6164   320   186  1838 11490  2716   111   118\n",
      "    10  3229   410   186   112    10  3229  2535     3    52   127  4601\n",
      "   117    27  8010  4944  1253   229   950   144   759   186    39  1151\n",
      "   133   336   412  1132   412   498     3  1717  2053    89  1438  4423\n",
      "  1019  9342  1782 26956  5625 11118   320   186  1838  1052   401   108\n",
      "   147   401 23909   447   111  5527 22291   713   186  1052   401     3\n",
      " 11124   185    39   317   320   787   181  4185    31  3007   809  5527\n",
      " 22291   713   132   770   401     2  1248   809   518    36   425   527\n",
      "  1973     2   931   320   644  2836 21675     5 11969     7  4450    45\n",
      "    39   655   111   401  9133  2840   186  5527 22291   713  7511    77\n",
      "   229    92  1973   181   401 23909   332  2002     9  3620   458   127\n",
      "   285   213   126  1353   160   527    28  1687  1443   446  4078  1915\n",
      "  2411     3    52   229    36   527  2094  1103   144  6044    95   213\n",
      " 11814   467    42    88   226   956    61   186   246   213   296   691\n",
      "    87   253    88   226  4091 10205     3    27 14380   127  3611   397\n",
      "    23  2195   229   456 16318  9326   478   186 21201     2    35   103\n",
      "   229    28   325   160   527    28  3631  1055   527  2942  1915   892\n",
      "   219    95  4078  2002     9 14380   969   285    66    10    65   446\n",
      "  6672   147   213  8102   720    78   925   374   194     2  1349  1248\n",
      "    72   446    28   194    95   213 11814     3     9   458  3088   213\n",
      " 24046   809   239   118    10  3229  2535    78    50  3698   843    70\n",
      "   926   102   103  1353    60  1595   691   644  2836 21675     5     2\n",
      "  1480   127  3611   321   332   111  9133  2840   669 22203   391  5527\n",
      " 22291   713 23740   509   320  1150   126   132   213  8584 15270    20\n",
      "   201  1480    23   576  1174   146  1200     3  8074 24042  2002 24857\n",
      "     5    25  2025   320   245   320   213  2737  7356   102  3620   447\n",
      "    25  7006   442     6 16002     4   592   379    27  2012   332    39\n",
      "    43  3872   320   186  1838   213  5116    78  1681   412  2935     2\n",
      "  1248 22366     5  3024   144   165     6  4944    21   181   892  1174\n",
      "    74  1200     2   931   320  3636 15052     3   804  7055 26508 21810\n",
      "    75     2  6683  1108   809   213  1314  8417  3250  3957 26956  1635\n",
      " 23280     2   127  3611 15055   132  3264   186  3805   229  1015     2\n",
      "   186    51  6672  1194   285  1782   200 11833 19962    16   578   285\n",
      " 17201     4   674     6  1047 13404   940  1536  1435 21847 12498   113\n",
      "  1782   871   315   229   869    11  6672   483   320  1151  1893    78\n",
      "   213  1973 16141   498     2    41   483   320   288   171  8068    28\n",
      "  5459   122   160   527   213  3007    39  1151   691  2137     2   186\n",
      "    41   483  5633   527   801    78   650   320 12227   756  4872   320\n",
      "   273   186  2754   320   124  1782   129    39  1151   996   320   172\n",
      "   285  8994   186  3636 15052  1435   981    38   132    31   363   320\n",
      " 18340  6672     2   320   399   105   191  2360  5663   186   320   191\n",
      "   103  1543  1019   105   320  1534 20214     5   181  7129  2002  3823\n",
      " 25946    21  6672    18   576   320  3698   320 24985    31 10469     3\n",
      " 26301     4 14178  1581   127  3611 17232   255    18  2274   139     2\n",
      "   139  2297  1019  5988  8271  3651  3620   809  9133  2840   122    41\n",
      "     7   165   996   809 11833 19962    16   691    42   390  1019    42\n",
      "   390   126  2002   639  7768  3789  5456  5455    84  1113  3611  4880\n",
      " 25903     4  2754     7     5   285    98   337  6164  1435 19715  1083\n",
      "   536  9133  2840 11822     3  4795  5225  1012     3 25433   320   172\n",
      "   285  1687  1443  1973  5459  1353  1378   103  2002 12930 24859   494\n",
      "   127  3611 21675     5    38 19715  1838   805     7     5  2840 11822\n",
      "  6821  2754   410   100  4418   320   124  3898   192   297  2326   127\n",
      "   117     9   982  1435    90 19581 10231    17     3  2157   103     7\n",
      "     5   141   107   117  4751     2   691   213   138     2    51     7\n",
      "   165  7115   805     7     5  2840  1019    28   335   390  2002    52\n",
      "  1041   102    28   194   527  1536 24046    78   213  8102   720   412\n",
      "   374   534   407 20301    20   132  1651  1353  1879    78 21526    16\n",
      "  1230     3 21675     5   111   723   186  2582   181  1803    78   213\n",
      "   644  2836     2   515  2836   181  1109  1057  9707  1060    70   186\n",
      "   213 23912     4     2  2840  3160   186   644 12661   471  1060  1568\n",
      " 10409  7356     3     9   542     6   442     6 16002     4 21526    16\n",
      "  1230   332    23  1459   824   104  1786    28 22760   351  1867    95\n",
      "   213 13528   246  5160   179   320   809   518   485     3  3074   186\n",
      "  2791   283    18 10161 15772 21597     5    95   213  1430   527   332\n",
      "     2  1480 11039  1024  1487     2  1144   341  3040   186  3730  1405\n",
      "   132   213 21526    16  1230  1781     3 15187  1838  5148 25203     5\n",
      "   186  8444  1337     2    86 11758 10570     4     2  7565  3620     2\n",
      " 18564  8302    47   186  1864  1619  6164  7356     3    27   676  1019\n",
      "  8735 14380   127  4601 27439  6050 13459  1628  3636 15052   186  1973\n",
      "  1095    18 25949   285    28   323   160   527   213  4091    39  1469\n",
      "   552   145   213  4078    37   119  2006   422   186  2360  4329  1435\n",
      "   573  4872  1060  1435  1879  4172 29725     4     1     0   644  2836\n",
      "     2   758 17062 21675     5     2  1711  1187   186  1109  1522  1060\n",
      "  1205 16346 27439  6774  1628 13499  1560   578   132  8584 15270    20\n",
      "   201   527   401    25    19  1393   132    55 16346 27439  6774  1628\n",
      " 11124   185  9961   320   669   391   787   186  4185  3007   809  5527\n",
      " 22291   713  1382 16346 27439  6774  1628 19435     5  1200   320   191\n",
      "   756     6  4078   955  3007    78  1895 16346 27439  6774  1628   644\n",
      "  2836 21675     5 14422  1407  1019 13146  6385    78    28   117   139\n",
      "  5955  1536   194  6053 27439  6774  1628   276 15052   127   669   391\n",
      "   447    62  1151   117  2111 17201    17    80 11822  2104     1]\n"
     ]
    }
   ],
   "source": [
    "print(input_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\n",
      " One of the country’s biggest rail terminals will be ‘effectively\n",
      "closed’ today because of over-running engineering works. There will be\n",
      "no trains in or out of King’s Cross in London due to delays to Network\n",
      "Rail works north of the station. The disruption comes on one of the\n",
      "busiest travel days of the year, as thousands of people try to return\n",
      "home after visiting family for Christmas. Scroll down for video .\n",
      "Services in and out of London Kings Cross station have been cancelled\n",
      "today, it has been announced . Frustration: Travellers at the London\n",
      "station, one of the busiest in the country, where services are\n",
      "cancelled . The disruptions at the station, which is managed by\n",
      "Network Rail, will affect those planning to travel on East Coast,\n",
      "First Hull Trains, Grand Central and Great Northern services. East\n",
      "Coast Trains made the announcement on its website yesterday evening,\n",
      "where it advised passengers to delay their travel if possible. It also\n",
      "said that a revised timetable is currently being developed and will be\n",
      "made available as soon as possible. Customers have been advised they\n",
      "need to start or finish their journey at Finsbury Park in north\n",
      "London, with at least one change of train because of the overrun works\n",
      "in the Holloway area. A reduced service to and from the station will\n",
      "go ahead as planned on Sunday, with trains  leaving up to 20 minutes\n",
      "earlier than normal from King's Cross. Last night Passenger Focus, the\n",
      "independent watchdog, described the delays as ‘frustrating’. East\n",
      "Coast made the announcement on their website and apologised for\n",
      "disruption on what is 'an already very busy travel day immediately\n",
      "following the Christmas break' They follow years of complaints from\n",
      "passengers that Britain’s railways effectively ‘shut down’ over the\n",
      "Christmas and New Year holiday. The chaos is heightened by the fact\n",
      "that the West Coast Main Line is already closed for engineering works\n",
      "over the Christmas period. The route is shut between London Euston and\n",
      "Hemel Hempstead in Hertfordshire until Monday, and also closed between\n",
      "Stafford and Crewe until Sunday. Last night travellers vented their\n",
      "frustration with the King’s Cross closure on social media, saying\n",
      "Network Rail had ‘explaining to do’. The station is the southern\n",
      "terminus of the East Coast Main Line, which provides services to major\n",
      "cities including Leeds, Newcastle and Edinburgh. East Coast spokesman,\n",
      "Paul Emberley, said: 'Network Rail has apologised to passengers for\n",
      "the inevitable delays to their travel plans on Saturday as a result of\n",
      "the overrunning engineering works. 'East Coast is particularly sorry\n",
      "too for the inconvenience to its customers as a result, on what we\n",
      "know is an already very busy travel day immediately following the\n",
      "Christmas break. Sources said that the station would effectively be\n",
      "closed  due to delays to works north of the station . 'For customers\n",
      "intending to start or finish their journey at King's Cross,\n",
      "consideration should be given to deferring travel plans to either\n",
      "Sunday or Monday. 'We're working hard over the holiday period to make\n",
      "the necessary adjustments to our timetable as a consequence, and to\n",
      "provide as much information as we can.' National Rail said on its\n",
      "website that services to and from King's Cross would be 'significantly\n",
      "disrupted' tomorrow. It said there will be no trains before 10am\n",
      "tomorrow and over the weekend, many trains will start or terminate at\n",
      "Doncaster, Peterborough, Stevenage or Finsbury Park. Meanwhile, First\n",
      "Hull Trains will run two trains in each direction each day to and from\n",
      "London St Pancras International instead of London Kings Cross. Great\n",
      "Northern are diverting some trains to and from Moorgate between 8.30am\n",
      "and 7.30pm. It said:  'A revised timetable is currently being\n",
      "developed and will be made available as soon as possible. Please check\n",
      "our website regularly for updates. 'Passengers travelling to and from\n",
      "central London may use London Underground services between Finsbury\n",
      "Park and central London. Customers will need to start or finish their\n",
      "journey at Finsbury Park in north London, with at least one change of\n",
      "train, according to East Coast Trains . 'Buses will run between London\n",
      "Kings Cross and Finsbury Park when there is no train or London\n",
      "Underground service.' The rail company said that the work was part of\n",
      "a £200 million Christmas investment programme. It is one of 300\n",
      "projects being undertaken over the holidays across 2,000 sites up and\n",
      "down the country by some 11,000 railway engineers. A spokesman said:\n",
      "'What has happened is really regrettable and unfortunate, but it is a\n",
      "small part of a massive amount of engineering investment taking place\n",
      "over Christmas.' The spokesman added that 4.5 million passengers use\n",
      "the railways on average every day, compared with two million a day\n",
      "over the holidays. The company confirmed the disruption at around\n",
      "8.30pm on its Twitter page - hours after it was first announced by\n",
      "East Coast Trains, which said: 'No service between Kings Cross &\n",
      "Finsbury Park tom due to track work in the Holloway area which has\n",
      "taken longer then expected. Apologies.' Hundreds were forced to take\n",
      "to the roads yesterday after rail services were virtually non-existent\n",
      "today . A reduced service will also operate to and from the terminal\n",
      "on Sunday as planned, with journeys possibly being re-timed or taking\n",
      "longer than expected, according to Network Rail. David Sidebottom,\n",
      "passenger director at the independent watchdog Passenger Focus, said:\n",
      "'Investment in maintenance and improvement is necessary, and we\n",
      "passengers understand that. 'But overrunning works that disrupt\n",
      "already-limited festive travel are frustrating. 'Our research is\n",
      "clear: passengers want to be kept on the train wherever possible, they\n",
      "want to know before buying a ticket if part of the journey will be by\n",
      "bus, and they want plenty of staff on hand to signpost where to go and\n",
      "what to do. 'We will be looking to see that operators and Network Rail\n",
      "are doing all in their power to alert passengers, to help them make\n",
      "alternative arrangements and to make it easy for them to claim refunds\n",
      "or compensation.' Several frustrated passengers have taken to Twitter\n",
      "to vent their anger. Liam Gladdy said: 'Something must have gone very,\n",
      "very wrong for @networkrail at Kings Cross if they're looking at\n",
      "overrunning by 2 days for 2 days work.' While Amelie Soleil wrote:\n",
      "'Ahhh what's that? All trains are cancelled coming though Kings Cross\n",
      "tomorrow. Brilliant. Nice to see that £200 train ticket was worth it.'\n",
      "Emily Clifton said: 'Trains all cancelled from King's Cross tomorrow\n",
      "literally what am i supposed to do,' while another user said 'The UK\n",
      "are so unprepared. Today it's just like 'Oh, by the way, we're closing\n",
      "King's Cross for a few days.' It comes after a day of travel\n",
      "disruption on the railways as every single major artery in Britain was\n",
      "closed on Boxing Day. Trains between England and Scotland or Wales on\n",
      "the East Coast, West Coast or Great Western mainlines - and the\n",
      "Midland, Cross Country and East Anglia lines remained shut yesterday.\n",
      "The near-non-existent Boxing Day service has returned this year\n",
      "despite a fierce political battle over the shutdown dating back to at\n",
      "least 2007. Labour and Tories have repeatedly traded accusations over\n",
      "the lack of service, which affects football fans, families without\n",
      "cars and shop workers in the Boxing Day sales. Apart from airport\n",
      "shuttles and Eurostar, only Chiltern, Scotrail, Southeastern and\n",
      "Southern ran trains yesterday. A Department for Transport spokesman\n",
      "said: ‘Network Rail and train companies have ensured that a large part\n",
      "of the railway will remain open during the Christmas/New Year period\n",
      "and alternative routes are provided where lines are\n",
      "closed.’<EOS><pad>EastCoast, First Hull Trains, Grand Central and\n",
      "Great Northern lines hit . Planned works in Holloway area of London\n",
      "were not completed in time . Customers advised to  start and finish\n",
      "journey at Finsbury Park instead . Thousands expected to make post-\n",
      "Christmas return journey on Saturday . East Coast Trains apologised\n",
      "for cancellation on a 'very busy travel day' National Rail said\n",
      "services would be 'significantly disrupted' tomorrow .<EOS>\n"
     ]
    }
   ],
   "source": [
    "print('Article:\\n\\n', detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    \"\"\"Create tensor from list of lists\"\"\"\n",
    "    return jnp.array(t)\n",
    "\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \"\"\"Display shape and tensor\"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takshshilarawat/opt/anaconda3/lib/python3.7/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotProductAttention(query, key, value, mask):\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    depth = query.shape[-1]\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
    "    \n",
    "    if mask is not None: \n",
    "        dots = jnp.where(mask, dots, jnp.full_like(dots, -1e9))\n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    dots = jnp.exp(dots - logsumexp)\n",
    "\n",
    "    attention = jnp.matmul(dots, value)\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    def compute_attention_heads(x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        seqlen = x.shape[1]\n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        x = jnp.reshape(x,(batch_size*n_heads,seqlen, d_head))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_self_attention(q, k, v):\n",
    "    mask_size = q.shape[-2]\n",
    "    mask = jnp.tril(jnp.ones((1, mask_size, mask_size), dtype=jnp.bool_), k=0)\n",
    "\n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "\n",
    "    def compute_attention_output(x):\n",
    "        seqlen = x.shape[1]\n",
    "        x = jnp.reshape(x,(-1, n_heads,seqlen,d_head))\n",
    "        x = jnp.transpose(x, (0,2,1,3))\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    \n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads',  compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
    "        \n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
    "        tl.Dense(d_feature) # Final dense layer\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    " \n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    feed_forward = [ \n",
    "        tl.LayerNorm(),\n",
    "        tl.Dense(d_ff),\n",
    "        ff_activation(), # Generally ReLU\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        tl.Dense(d_model),\n",
    "        tl.Dropout(rate=dropout, mode=mode)\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "      tl.Residual(\n",
    "           tl.LayerNorm(),\n",
    "          causal_attention,\n",
    "          tl.Dropout(rate=dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "\n",
    "    positional_encoder = [ \n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    return tl.Serial(\n",
    "    \n",
    "        tl.ShiftRight(mode=mode),\n",
    "        positional_encoder,\n",
    "        decoder_blocks,\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        tl.Dense(vocab_size),\n",
    "        tl.LogSoftmax()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
    "\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function \n",
    "      optimizer=trax.optimizers.Adam(learning_rate=0.01), \n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=10\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy\n",
    "    )\n",
    "\n",
    "    loop = training.Loop(TransformerLM(d_model=4,\n",
    "                                       d_ff=16,\n",
    "                                       n_layers=1,\n",
    "                                       n_heads=2,\n",
    "                                       mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f ~/model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model architecture\n",
    "model = TransformerLM(mode='eval')\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_symbol(cur_output_tokens, model):\n",
    "\n",
    "    token_length = len(cur_output_tokens)\n",
    "    \n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this 'None'! This is a way of setting the batch dim\n",
    "\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    return int(np.argmax(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(input_sentence, model):\n",
    "\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]\n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        generated_output.append(cur_output)\n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
